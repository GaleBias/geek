<audio title="13｜高性能网络实战（下）：如何完善负载均衡器？" src="https://static001.geekbang.org/resource/audio/d1/19/d13bbcca839bbbc1b9141c7a2e1d9c19.mp3" controls="controls"></audio> 
<p>你好，我是倪朋飞。</p><p>上一讲，我带你使用 sockops 和 sk_msg 等套接字 eBPF 程序，在内核态对套接字进行转发，提升了负载均衡的性能。</p><p>对于网络优化来说，除了套接字 eBPF 程序，XDP 程序和 TC 程序也可以用来优化网络的性能。特别是 XDP 程序，由于它在 Linux 内核协议栈之前就可以处理网络包，在负载均衡、防火墙等需要高性能网络的场景中已经得到大量的应用。</p><p>XDP 程序在内核协议栈初始化之前运行，这也就意味着在 XDP 程序中，你并不能像在 sockops 等程序中那样直接获得套接字的详细信息。使用 XDP 程序加速负载均衡，通常也就意味着需要从头开发一个负载均衡程序。这是不是说 XDP 的使用就特别复杂，需要重新实现内核协议栈的很多逻辑呢？不要担心，<strong>XDP 处理过的数据包还可以正常通过内核协议栈继续处理，所以你只需要在 XDP 程序中实现最核心的网络逻辑就可以了</strong>。</p><p>今天，我就以 XDP 程序为例，带你继续优化和完善负载均衡器的性能。</p><h2>案例准备</h2><p>跟上一讲类似，为了方便环境的重现，负载均衡器、Web 服务器以及客户端都还是运行在容器中，它们的 IP 和 MAC 等基本信息如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/e9/15/e923026f577f7b991be2610734f9e415.jpg?wh=1920x1706" alt="图片"><br>
执行下面的命令，启动这几个容器：</p><pre><code class="language-bash"># Webserver
docker run -itd --name=http1 --hostname=http1 feisky/webserver
docker run -itd --name=http2 --hostname=http2 feisky/webserver

# Client
docker run -itd --name=client alpine

# LB
docker run -itd --name=lb --privileged -v /sys/kernel/debug:/sys/kernel/debug alpine
</code></pre><!-- [[[read_end]]] --><blockquote>
<p>小提示：在默认安装的 Docker 环境中，假如你没有运行其他容器，运行上述命令后得到的 IP 地址跟图中是相同的。</p>
</blockquote><p>注意，我们把作为负载均衡器的 Nginx 换成了基于 alpine 镜像的 SHELL 容器，并且以特权容器的方式运行，以便有足够的权限加载并运行 XDP 程序。</p><p>把 XDP 程序放入容器中，除了容易复现案例环境之外，在开发和调试 XDP 程序的过程中也不会影响主机的网络（否则，错误的 XDP 程序可能导致主机网络中断，进而也会影响远程 SSH 连接）。</p><p>由于负载均衡容器只启动了一个 SHELL 环境，并没有运行真正的负载均衡服务。此时，访问负载均衡器的 TCP 80 端口会直接失败。你可以运行下面的命令到客户端容器中验证（<code>/ #</code> 后的命令表示在容器终端中运行）：</p><pre><code class="language-bash">docker exec -it client sh
/ # apk add curl --update
/ # curl "http://172.17.0.5"
curl: (7) Failed to connect to 172.17.0.5 port 80 after 1 ms: Connection refused
</code></pre><p>案例所需的容器环境启动完毕后，接下来我们再来看看，如何使用 XDP 开发一个负载均衡服务。由于需要把 XDP 字节码放到容器中运行，本着最小依赖的原则，我们将使用 libbpf 作为 XDP 的基础库，这样只需要把编译后的二进制文件放入容器中即可运行。</p><h2>如何用 XDP 开发一个负载均衡器？</h2><p>还记得我在<a href="https://time.geekbang.org/column/article/484372"> 08 讲</a> 中提到过的基于 libbpf 开发 eBPF 程序的基本步骤吗？不记得也没关系，我们再来回顾一下。 libbpf 的使用通常分为以下几步：</p><ol>
<li>开发 eBPF 程序，并把源文件命名为 <code>&lt;程序名&gt;.bpf.c</code>；</li>
<li>编译 eBPF 程序为字节码，然后再调用&nbsp;<code>bpftool gen skeleton</code>&nbsp;为 eBPF 字节码生成脚手架头文件；</li>
<li>开发用户态程序，引入生成的脚手架头文件后，加载 eBPF 程序并挂载到相应的内核事件中。</li>
</ol><p>接下来，我们就按这几个步骤来开发 XDP 负载均衡程序。</p><h3>开发 XDP eBPF 程序</h3><p>第一步是开发一个运行在内核态的 eBPF 程序。参考内核中 BPF_PROG_TYPE_XDP 程序类型的<a href="https://elixir.bootlin.com/linux/v5.13/source/include/linux/bpf_types.h#L11">定义格式</a>，它的参数类型为 <a href="https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L5283">struct xdp_md</a>：</p><pre><code class="language-c++">#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type)

BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp,
       struct xdp_md, struct xdp_buff)
</code></pre><p>因而，你就可以使用如下的格式来定义这个 XDP 程序：</p><pre><code class="language-c++">SEC("xdp")
int xdp_proxy(struct xdp_md *ctx)
{
  // TODO: 添加XDP负载均衡逻辑
}
</code></pre><p>这段代码中，<code>SEC("xdp")</code> 表示程序的类型为 XDP 程序。你可以在 libbpf 中 <a href="https://github.com/libbpf/libbpf/blob/master/src/libbpf.c#L8599-L8675">section_defs</a>找到所有 eBPF 程序类型对应的段名称格式。</p><p>参考 <code>linux/bpf.h</code> 头文件中 <a href="https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L5283">struct xdp_md</a> 的定义格式，你可以发现，它比上一讲用到的 <a href="https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/bpf.h#L5506">struct bpf_sock_ops</a> 简单多了，只包含了如下的几个字段：</p><pre><code class="language-c++">struct xdp_md {
  __u32 data;
  __u32 data_end;
  __u32 data_meta;
  /* Below access go through struct xdp_rxq_info */
  __u32 ingress_ifindex; /* rxq-&gt;dev-&gt;ifindex */
  __u32 rx_queue_index;  /* rxq-&gt;queue_index  */
  __u32 egress_ifindex;  /* txq-&gt;dev-&gt;ifindex */
};
</code></pre><p>从 <code>struct xdp_md</code> 的定义中你可以看到，所有的字段都是整型数值，其中前三个表示数据指针信息（包括开始位置、结束位置、元数据位置），而后三个表示关联网卡的信息（包括入口网卡、入口网卡队列以及出口网卡的编号）。</p><p>由于 <code>struct xdp_md</code> 中并不包含 skb 数据结构，在 XDP 程序中，你只能通过 <code>data</code> 和 <code>data_end</code> 这两个指针去访问网络报文数据。而要想利用原始网络数据指针来访问网络数据，就需要你了解 TCP/IP 网络报文的基本格式。</p><p>为了方便你理解，我画了一张图，标记了以太网头、IP 头以及 TCP 头等相对于 <code>struct xdp_md</code> 中数据指针的位置关系：</p><p><img src="https://static001.geekbang.org/resource/image/yy/91/yy7887570f06c1d075eb31701924e791.jpg?wh=1920x577" alt="图片"><br>
有了这些对应关系，要访问 TCP/IP 协议某一层的头结构，就可以使用开始指针 <code>data</code> 再加上它之前所有头结构大小的偏移。</p><p>比如，对于以太网头，它的位置跟开始位置 <code>data</code> 是相同的，因而你就可以使用下面的方式，把它转换为指针格式进行访问：</p><pre><code class="language-c++">void *data = (void *)(long)ctx-&gt;data;
void *data_end = (void *)(long)ctx-&gt;data_end;

struct ethhdr *eth = data;
if (data + sizeof(struct ethhdr) &gt; data_end)
{
  return XDP_ABORTED;
}
</code></pre><p>为了帮助 eBPF 校验器验证数据访问的合法性，在访问以太网头数据结构 <code>struct ethhdr</code> 之前，你需要检查数据指针是否越界。如果检查失败，就要返回一个错误（这儿返回的 <code>XDP_ABORTED</code> 表示丢弃数据包并记录错误行为以便排错）。</p><p>了解了太网头的访问格式之后，IP 头的访问也是类似的。在开始指针 <code>data</code> 之后加上太网头数据结构的长度偏移，就是 IP 头所指向的位置。拿到 IP 头之后，你还可以对网络数据进行初步的校验，比如忽略 IPv6、UDP 等我们不关心的数据，只处理 TCP 数据包，代码如下：</p><pre><code class="language-c++">struct iphdr *iph = data + sizeof(struct ethhdr);
if (data + sizeof(struct ethhdr) + sizeof(struct iphdr) &gt; data_end)
{
  return XDP_ABORTED;
}

if (eth-&gt;h_proto != bpf_htons(ETH_P_IP))
{
  return XDP_PASS;
}

if (iph-&gt;protocol != IPPROTO_TCP)
{
  return XDP_PASS;
}
</code></pre><p>这段代码中返回的 <code>XDP_PASS</code> 表示把网络包传递给内核协议栈，内核协议栈接收到网络包后，按正常流程继续处理。</p><p>进行了基本的校验之后，再接下来就是实现负载均衡的逻辑了。由于我们想要实现的是一个四层负载均衡，试想一下，负载均衡器收到客户端的请求之后，需要把目的地址（包括 IP 和 MAC）替换成后端 Webserver 的地址，再重新发到内核协议栈中继续处理。</p><p>参考内核中<a href="https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/if_ether.h#L165">以太网头</a>和 <a href="https://elixir.bootlin.com/linux/v5.13/source/include/uapi/linux/ip.h#L86">IP 头</a>的定义格式， IP 地址都是以 <code>__be16</code> 类型的大端格式存储，而 MAC 地址则是以字符数组 <code>unsigned char h_dest[6]</code> 的形式存储。因而，MAC 地址可以直接通过数据下标进行访问，而我们案例开始时列出的 IP 地址，还需要先转换为 <code>__be16</code> 格式的大端存储格式。</p><p>如果你还不熟悉 IP 地址的转换方法，那可以参考下面的程序，调用 <code>inet_addr()</code> 库函数帮你完成转换：</p><pre><code class="language-c++">#include &lt;stdio.h&gt;
#include &lt;arpa/inet.h&gt;

int main() {
  unsigned int a1 = inet_addr("172.17.0.2");
  unsigned int a2 = inet_addr("172.17.0.3");
  unsigned int a3 = inet_addr("172.17.0.4");
  unsigned int a4 = inet_addr("172.17.0.5");
  printf("0x%x 0x%x 0x%x 0x%x\n", a1, a2, a3, a4);
}
</code></pre><p>为了方便你理解接下来的程序，我把转换后的容器地址信息整理成了一个表格，你可以在后续的开发和问题排查过程中参考：</p><p><img src="https://static001.geekbang.org/resource/image/d3/bb/d31389c380dc3dddf2128495d35b6ebb.jpg?wh=2284x1510" alt=""></p><p>接下来，就是负载均衡的实现过程了，也就是根据请求的来源，把目的地址修改为 Webserver 的地址。下面的代码展示的就是一个最简单的负载均衡实现逻辑：</p><pre><code class="language-c++">/* 1. 常量定义 */
#define CLIENT_IP 0x40011ac
#define LOADBALANCER_IP 0x50011ac
#define ENDPOINT1_IP 0x20011ac
#define ENDPOINT2_IP 0x30011ac
#define CLIENT_MAC_SUFFIX 0x04
#define LOADBALANCER_MAC_SUFFIX 0x05
#define ENDPOINT1_MAC_SUFFIX 0x02
#define ENDPOINT2_MAC_SUFFIX 0x03

/* 2. 从客户端发送过来的请求，目的地址改为后端 Webserver 的地址 */
if (iph-&gt;saddr == CLIENT_IP)
{
  iph-&gt;daddr = ENDPOINT1_IP;
  eth-&gt;h_dest[5] = ENDPOINT1_MAC_SUFFIX; /* Only need to update the last byte */

  /* 模拟从两个Webserver随机选择 */
  if ((bpf_ktime_get_ns() &amp; 0x1) == 0x1)
  {
    iph-&gt;daddr = ENDPOINT2_IP;
    eth-&gt;h_dest[5] = ENDPOINT2_MAC_SUFFIX;
  }
}
else /* 3. 反之，目的地址改为客户端 */
{
  iph-&gt;daddr = CLIENT_IP;
  eth-&gt;h_dest[5] = CLIENT_MAC_SUFFIX;
}

/* 4. 修改原地址为LB地址 */
iph-&gt;saddr = LOADBALANCER_IP;
eth-&gt;h_source[5] = LOADBALANCER_MAC_SUFFIX;
</code></pre><p>这段代码中各部分的含义如下：</p><ul>
<li>第 1 部分，将容器地址信息定义为常量，方便后续引用和理解。注意，MAC 地址是一个包含6 个元素的数组，而前 5 个元素的值都是相同的。因而，在更新目的 MAC 地址时，只需要更新最后一个元素即可。所以，常量定义里面也只包含了最后一个字节的值。</li>
<li>第 2 部分，对于从客户端发送过来的请求，将目的地址改为后端 Webserver 的地址。由于只有两个后端 Webserver，这儿使用时间戳最后一位的值模拟它们的随机选择过程。</li>
<li>第 3 部分，对于从 Webserver 发送过来的响应，目的地址改为客户端地址。</li>
<li>第 4 部分，将原地址都改为负载均衡器的地址。</li>
</ul><p>到这里， eBPF 程序是不是已经开发好了呢？其实，如果你了解过 IP 协议的基本原理，你就知道还有一个步骤也是非常重要的：由于修改了 IP 头的数据，IP 头的校验和（checksum）就需要重新计算，否则网络包会被内核直接丢弃。</p><p>如果你不熟悉 IP 头校验和的计算方法也没关系，你可以很容易从成熟的开源项目中查找到相关的计算方法。比如，参考 Facebook 开源的 <a href="https://github.com/facebookincubator/katran/blob/main/katran/lib/bpf/csum_helpers.h#L30">Katran</a>，你可以定义如下的 <code>ipv4_csum()</code> 函数来计算校验和：</p><pre><code class="language-c++">static __always_inline __u16 csum_fold_helper(__u64 csum)
{
  int i;
#pragma unroll
  for (i = 0; i &lt; 4; i++)
  {
  if (csum &gt;&gt; 16)
    csum = (csum &amp; 0xffff) + (csum &gt;&gt; 16);
  }
  return ~csum;
}

static __always_inline __u16 ipv4_csum(struct iphdr *iph)
{
  iph-&gt;check = 0;
  unsigned long long csum = bpf_csum_diff(0, 0, (unsigned int *)iph, sizeof(struct iphdr), 0);
  return csum_fold_helper(csum);
}
</code></pre><p>关于校验和的具体算法，你可以参考 TCP/IP 协议相关的原理书籍（如《TCP/IP详解》）来理解，这里我就不详细展开了。</p><p>有了校验和的计算方法之后，最后更新 IP 头的 checksum，再返回 <code>XDP_TX</code> 把数据包从原网卡发送出去，交给内核去转发就可以了。下面展示的就是更新校验和的实现方法：</p><pre><code class="language-c++">SEC("xdp")
int xdp_proxy(struct xdp_md *ctx)
{
  ...
  /* 重新计算校验和 */
  iph-&gt;check = ipv4_csum(iph);

  /* 把数据包从原网卡发送出去 */
  return XDP_TX;
}
</code></pre><p>把上述代码保存到一个文件 <code>xdp-proxy.bpf.c</code> 中，就完成了 XDP eBPF 程序的开发（你还可以在 <a href="https://github.com/feiskyer/ebpf-apps/blob/main/loadbalancer/xdp/xdp-proxy.bpf.c">GitHub</a> 中找到完整的代码）。</p><h3>编译并生成脚手架头文件</h3><p>有了 XDP 程序之后，接下来的第二步就比较简单了。我们只需要执行下面的 <code>clang</code> 命令，把 XDP 程序编译成字节码，再执行 <code>bpftool gen skeleton</code> 命令生成脚手架头文件即可：</p><pre><code class="language-bash">clang -g -O2 -target bpf -D__TARGET_ARCH_x86 -I/usr/include/x86_64-linux-gnu -I. -c xdp-proxy.bpf.c -o xdp-proxy.bpf.o
bpftool gen skeleton xdp-proxy.bpf.o &gt; xdp-proxy.skel.h
</code></pre><h3>开发用户态程序</h3><p>对于第三步用户态程序的开发，它的基本流程跟 <a href="https://time.geekbang.org/column/article/484372">08 讲</a> 中的内核跟踪案例是类似的，也是需要引入脚手架头文件、增大 RLIMIT_MEMLOCK、初始化并加载 BPF 字节码，最后再挂载 XDP 程序这几个步骤。忽略错误处理步骤，最核心的实现步骤如下所示：</p><pre><code class="language-c++">// 1. 引入脚手架头文件
#include "xdp-proxy.skel.h"

// C语言主函数
int main(int argc, char **argv)
{
    // 2. 增大 RLIMIT_MEMLOCK（默认值通常太小，不足以存入BPF映射的内容）
    struct rlimit rlim_new = {
      .rlim_cur = RLIM_INFINITY,
      .rlim_max = RLIM_INFINITY,
    };
    err = setrlimit(RLIMIT_MEMLOCK, &amp;rlim_new);

    // 3. 初始化BPF程序
    struct xdp_proxy_bpf *obj = xdp_proxy_bpf__open();

    // 4. 加载BPF字节码
    err = xdp_proxy_bpf__load(obj);

    // 5. TODO: 挂载XDP程序到eth0网卡
}
</code></pre><p>这段代码中，前面 4 个步骤跟<a href="https://time.geekbang.org/column/article/484372"> 08 讲</a> 中的<a href="https://time.geekbang.org/column/article/484372">内核跟踪案例</a>是一样的，这儿不再详细展开。</p><p>而对于第 5 步的挂载过程，我在<a href="https://time.geekbang.org/column/article/483364"> 06 讲</a> 中曾经提到，你可以使用 <code>ip link</code> 命令来挂载 XDP 程序。当时讲到的是在主机中挂载 XDP 的步骤，而在容器中的步骤其实也是一样的（注意，在容器中的虚拟网卡上，只支持以通用模式挂载）。</p><p>下面的代码展示的就是把 XDP 字节码复制到负载均衡容器并挂载到 eth0 网卡的过程：</p><pre><code class="language-bash"># 复制字节码到容器中
docker cp xdp-proxy.bpf.o lb:/

# 在容器中安装iproute2命令行工具
docker exec -it lb apk add iproute2 --update

# 在容器中挂载XDP程序到eth0网卡
docker exec -it lb ip link set dev eth0 xdpgeneric object xdp-proxy.bpf.o sec xdp
</code></pre><p>除了使用命令行工具之外，你还可以在用户态程序中调用库函数来挂载 XDP 程序，从而避免引入额外的命令行工具依赖（比如，不再需要安装 iproute2 系统工具）。</p><p>libbpf 提供了一个 <code>bpf_set_link_xdp_fd(int ifindex, int fd, __u32 flags)</code> 函数，可用于把 XDP 程序挂载到网卡。这个函数需要网卡序号和 XDP 程序文件描述符作为参数，查询这些参数并挂载 XDP 的过程如下所示：</p><pre><code class="language-c++">    unsigned int ifindex = if_nametoindex("eth0");
    int prog_id = bpf_program__fd(obj-&gt;progs.xdp_proxy);
    err = bpf_set_link_xdp_fd(ifindex, prog_id, XDP_FLAGS_UPDATE_IF_NOEXIST|XDP_FLAGS_SKB_MODE);
</code></pre><p>这段代码中，挂载参数标志 <code>XDP_FLAGS_SKB_MODE</code> 等同于 <code>ip link</code> 命令中的 <code>xdpgeneric</code>，表示以通用模式挂载。</p><p>把上述代码保存到 <code>xdp-proxy.c</code> 文件中（你还可以在 <a href="https://github.com/feiskyer/ebpf-apps/blob/main/loadbalancer/xdp/xdp-proxy.c">GitHub</a> 中找到完整的代码），再执行下面的编译命令，就可以将其编译为静态链接的可执行文件。采用静态链接的一个好处是容易在容器中分发，只需要把最终的二进制文件放入容器中即可运行，不再需要安装额外的依赖环境。</p><pre><code class="language-bash">clang -g -O2 -Wall -I. -c xdp-proxy.c -o xdp-proxy.o
clang -Wall -O2 -g xdp-proxy.o -static -lbpf -lelf -lz -o xdp-proxy
</code></pre><p>到这里，完整的 eBPF 程序就开发好了。它是不是可以正常工作呢？如果可以正常工作，性能又会怎么样？接下来，我们把它放到容器中测试一下看看。</p><h3>性能测试</h3><p>在终端中执行下面的 docker 命令，把 XDP 程序复制到负载均衡容器中，并执行 XDP 程序：</p><pre><code class="language-bash"># 复制XDP程序到容器
docker cp xdp-proxy lb:/

# 在容器中加载XDP程序
docker exec -it lb /xdp-proxy
</code></pre><p>然后，进入客户端容器终端中，执行 <code>apk</code> 命令安装 <code>curl</code> 和 <code>wrk</code> 工具，接着再使用 <code>curl</code> 访问负载均衡器：</p><pre><code class="language-bash">docker exec -it client sh

# (以下命令运行在client容器中)
/ # curl "http://172.17.0.5"
</code></pre><p>如果你看到 <code>Hostname: http1</code> 或者 <code>Hostname: http12</code> 的输出，说明 XDP 已经成功运行，并且它的负载均衡功能也是正常的。</p><p>接下来，继续在客户端容器终端中执行 <code>wrk</code> 命令，给负载均衡器做个性能测试：</p><pre><code class="language-bash"># (以下命令运行在client容器中)

# 安装wrk工具
/ # apk add curl wrk --update

# 执行性能测试
/ # wrk -c100 "http://172.17.0.5"
</code></pre><p>稍等一会，你会看到如下的输出（在你的环境下可能看到不同数值，具体的性能指标取决于运行环境和配置）：</p><pre><code class="language-bash">Running 10s test @ http://172.17.0.5
  2 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     6.37ms   11.17ms 295.43ms   98.97%
    Req/Sec     9.09k   422.06    10.09k    75.00%
  180889 requests in 10.02s, 31.39MB read
Requests/sec:  18048.65
Transfer/sec:      3.13MB
</code></pre><p>从输出中你可以看到，平均每秒请求数是 18048，每个线程的平均延迟是 6.37ms。回顾一下 <a href="https://time.geekbang.org/column/article/485702?cid=100104501">12 讲</a> 中的套接字 eBPF 程序的性能测试结果，它的平均每秒请求数是 15300，而每个线程的平均延迟是 6.88ms。这说明，相比套接字程序，XDP 程序在平均每秒请求数上提升了 18%。</p><p>最后，不要忘记清理今天的案例环境。由于所有服务都运行在容器中，我们只需要执行下面的命令，删除今天创建的所有容器，即可完成清理工作：</p><pre><code class="language-bash">docker rm -f lb client http1 http2
</code></pre><h2>小结</h2><p>今天，我带你使用 libbpf 开发了一个基于 XDP 的负载均衡服务。</p><p>XDP 程序在网络驱动程序刚刚收到数据包的时候触发执行。由于还未分配内核 SKB 数据结构，XDP 程序只能根据 TCP/IP 协议的封包格式，从原始网络包中提取所需协议层的数据，进而再按照需要进行改写或转发。XDP 程序修改过的数据包可以转发给相同或不同的网卡，再交给内核协议栈继续处理；或者，跟处理逻辑无关的包不做任何处理，直接交给内核协议栈进行处理。</p><p>今天的案例把 XDP 程序挂载到了容器的虚拟网卡上，由于采用了通用模式挂载，它的执行过程其实还是在内核中运行的。在实际生产环境中，你可以以原生模式或卸载模式把 XDP 程序挂载到支持 XDP 的网卡上，从而获得最优的网络性能。</p><h2>思考题</h2><p>最后，我想邀请你来聊一聊：</p><ol>
<li>在今天的案例中，我们把 XDP 程序挂载到了负载均衡容器中的网卡上。如果把它直接挂载到主机的 eth0 网卡，会有什么样的现象？</li>
<li>针对今天的负载均衡场景，还有哪些方法可以进一步优化 XDP 程序的性能？</li>
</ol><p>期待你在留言区和我讨论，也欢迎把这节课分享给你的同事、朋友。让我们一起在实战中演练，在交流中进步。</p>
<style>
    ul {
      list-style: none;
      display: block;
      list-style-type: disc;
      margin-block-start: 1em;
      margin-block-end: 1em;
      margin-inline-start: 0px;
      margin-inline-end: 0px;
      padding-inline-start: 40px;
    }
    li {
      display: list-item;
      text-align: -webkit-match-parent;
    }
    ._2sjJGcOH_0 {
      list-style-position: inside;
      width: 100%;
      display: -webkit-box;
      display: -ms-flexbox;
      display: flex;
      -webkit-box-orient: horizontal;
      -webkit-box-direction: normal;
      -ms-flex-direction: row;
      flex-direction: row;
      margin-top: 26px;
      border-bottom: 1px solid rgba(233,233,233,0.6);
    }
    ._2sjJGcOH_0 ._3FLYR4bF_0 {
      width: 34px;
      height: 34px;
      -ms-flex-negative: 0;
      flex-shrink: 0;
      border-radius: 50%;
    }
    ._2sjJGcOH_0 ._36ChpWj4_0 {
      margin-left: 0.5rem;
      -webkit-box-flex: 1;
      -ms-flex-positive: 1;
      flex-grow: 1;
      padding-bottom: 20px;
    }
    ._2sjJGcOH_0 ._36ChpWj4_0 ._2zFoi7sd_0 {
      font-size: 16px;
      color: #3d464d;
      font-weight: 500;
      -webkit-font-smoothing: antialiased;
      line-height: 34px;
    }
    ._2sjJGcOH_0 ._36ChpWj4_0 ._2_QraFYR_0 {
      margin-top: 12px;
      color: #505050;
      -webkit-font-smoothing: antialiased;
      font-size: 14px;
      font-weight: 400;
      white-space: normal;
      word-break: break-all;
      line-height: 24px;
    }
    ._2sjJGcOH_0 ._10o3OAxT_0 {
      margin-top: 18px;
      border-radius: 4px;
      background-color: #f6f7fb;
    }
    ._2sjJGcOH_0 ._3klNVc4Z_0 {
      display: -webkit-box;
      display: -ms-flexbox;
      display: flex;
      -webkit-box-orient: horizontal;
      -webkit-box-direction: normal;
      -ms-flex-direction: row;
      flex-direction: row;
      -webkit-box-pack: justify;
      -ms-flex-pack: justify;
      justify-content: space-between;
      -webkit-box-align: center;
      -ms-flex-align: center;
      align-items: center;
      margin-top: 15px;
    }
    ._2sjJGcOH_0 ._10o3OAxT_0 ._3KxQPN3V_0 {
      color: #505050;
      -webkit-font-smoothing: antialiased;
      font-size: 14px;
      font-weight: 400;
      white-space: normal;
      word-break: break-word;
      padding: 20px 20px 20px 24px;
    }
    ._2sjJGcOH_0 ._3klNVc4Z_0 {
      display: -webkit-box;
      display: -ms-flexbox;
      display: flex;
      -webkit-box-orient: horizontal;
      -webkit-box-direction: normal;
      -ms-flex-direction: row;
      flex-direction: row;
      -webkit-box-pack: justify;
      -ms-flex-pack: justify;
      justify-content: space-between;
      -webkit-box-align: center;
      -ms-flex-align: center;
      align-items: center;
      margin-top: 15px;
    }
    ._2sjJGcOH_0 ._3Hkula0k_0 {
      color: #b2b2b2;
      font-size: 14px;
    }
</style><ul><li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/0f/5e/96/a03175bc.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>莫名</span>
  </div>
  <div class="_2_QraFYR_0">1. 挂在宿主机的 eth0 网卡，会导致宿主机网络受影响，最直接的影响是来自外部的 ssh 连接异常，之后再执行 ssh 也登不进宿主机。（原因是源 IP 不是 CLIENT_IP 时将目的 IP 直接修改为 CLIENT_IP，源 IP 无论如何均修改为 LOADBALANCER_IP，进来的数据包无法正常被接收与响应）<br><br>2. 目前的实现有些 hardcode，不利于扩展，负载均衡器通常运行配置多个 vip，每个 vip 对应若干真正的后端服务。觉得改进措施可以用 BPF map 类型存储，最好有个文本格式的配置文件，程序加载前解析这个配置文件，填充 BPF map。xdp 程序中根据 key（比如 vip 或者 UUID）查找真正的后端服务，如果能够找到则从中读取 IP、Mac 等信息。</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 非常赞的答案👍 如果能把这些思路再都实现了就更好了😊</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-02-15 09:35:35</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/28/6d/5f/a937f7f1.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>乌拉呆zyb</span>
  </div>
  <div class="_2_QraFYR_0">倪老师，请问为什么将XDP eBPF程序挂载上去之后，优化效果反而大大地下降了？优化前 Requests&#47;sec 是7500左右；XDP优化后 Requests&#47;sec 是250 左右；代码都是github上的代码；两个版本的都试过了，都是反向优化，不知道为啥 ^~^</div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-05-04 14:28:25</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/0f/f6/2f/d3f1dae6.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>L33K</span>
  </div>
  <div class="_2_QraFYR_0">如果一个上层的大包被拆成一个多个包发过来，目前这种负载均衡方式是不是可能就有问题了？</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 嗯 XDP是有一些限制的，具体可以参考这个PPT： https:&#47;&#47;lpc.events&#47;event&#47;11&#47;contributions&#47;939&#47;attachments&#47;771&#47;1551&#47;xdp-multi-buff.pdf</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-02-27 23:32:15</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83erBT5LK5f0w6MHCyX7Tuc1aytDlSazDkFPibwZ113Luz8qLviccotz4k3oIePQzrZhHEBWSDKUIjw7A/132"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>aith</span>
  </div>
  <div class="_2_QraFYR_0">XDP程序把数据包随机调度到某个Webserver，没有会话保持，会不会导致同一次请求的数据包，发送到不同的后端Webserver上面,从而不能正常相应？</div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-05-24 15:49:14</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/28/6d/5f/a937f7f1.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>乌拉呆zyb</span>
  </div>
  <div class="_2_QraFYR_0">倪老师，还想请教您一个问题： 加载XDP eBPF程序后，curl &quot;http:&#47;&#47;172.17.0.5&quot; 测试正常，有负载均衡能力；但 wrk -c 100 &quot;http:&#47;&#47;172.17.0.5&quot; 的测试结果中有Socker errors的连接超时的报错如下：<br>&#47; # wrk -c 100 &quot;http:&#47;&#47;172.17.0.5&quot;<br>Running 10s test @ http:&#47;&#47;172.17.0.5<br>  2 threads and 100 connections<br>  Thread Stats   Avg      Stdev     Max   +&#47;- Stdev<br>    Latency   274.75ms  384.76ms   1.70s    86.88%<br>    Req&#47;Sec   127.44     86.42   530.00     69.19%<br>  2640 requests in 10.09s, 469.22KB read<br>  Socket errors: connect 0, read 0, write 0, timeout 56<br>Requests&#47;sec:    261.64<br>Transfer&#47;sec:     46.50KB<br>&#47; # </div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-05-04 16:22:10</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/2d/27/c5/d4d00da2.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>为了维护世界和平</span>
  </div>
  <div class="_2_QraFYR_0">倪老师你好，使用XDP 速度慢了，这大概什么原因呢<br>&#47; # wrk -c100 &quot;http:&#47;&#47;172.17.0.5&quot;<br>Running 10s test @ http:&#47;&#47;172.17.0.5<br>  2 threads and 100 connections<br>  Thread Stats   Avg      Stdev     Max   +&#47;- Stdev<br>    Latency   272.26ms  386.46ms   1.70s    86.32%<br>    Req&#47;Sec   122.51     90.04   580.00     73.65%<br>  2483 requests in 10.06s, 441.31KB read<br>  Socket errors: connect 0, read 0, write 0, timeout 53<br>Requests&#47;sec:    246.73<br>Transfer&#47;sec:     43.85KB<br></div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-07-20 09:25:19</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/12/55/54/97419b60.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>codejw</span>
  </div>
  <div class="_2_QraFYR_0">老师，如果有多个xdp程序如何挂载呢，我有多个.o都挂载到xdp</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 如果是不同的网卡，多次调用挂载和加载的函数或者用 ip link set 命令都可以。但如果想挂载多个XDP程序到相同的网卡，那就需要5.10新增的freplace类型，具体细节可以参考 https:&#47;&#47;lpc.events&#47;event&#47;7&#47;contributions&#47;671&#47;attachments&#47;561&#47;992&#47;xdp-multiprog.pdf</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-02-16 23:26:07</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/16/cd/db/7467ad23.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>Bachue Zhou</span>
  </div>
  <div class="_2_QraFYR_0">这个程序对 libbpf 的版本要求很高啊，我这边用的是 Ubuntu 22.04 都不行，很多 symbol 找不到的。</div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2023-04-13 17:33:35</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/30/6c/de/693fce23.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>Aiolos</span>
  </div>
  <div class="_2_QraFYR_0">原生nginx和sock_ops+sk_msg优化均是2000QPS，但是使用XDP优化后降到了400QPS，请问这是什么原因，如何修正</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 很有可能是用了虚拟机导致的。案例中使用的是通用挂载模式，实际生产中使用原生模式或卸载模式才能获得比较好的加速效果。</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2023-02-02 11:19:00</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://thirdwx.qlogo.cn/mmopen/vi_32/pNKoOAa1QXibrykHNXibW4tyaIIhicocPGXtcVnEianCyOQY9bl0P2JQ3wSialUaolcLVEWycCEBz1Oe4Tj4yghH9yw/132"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>Geek_5aa343</span>
  </div>
  <div class="_2_QraFYR_0">libbpf中的section_defs链接更新下：https:&#47;&#47;github.com&#47;libbpf&#47;libbpf&#47;blob&#47;master&#47;src&#47;libbpf.c#L9003-L9080</div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-05-15 23:13:00</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/12/ce/b4/bb5d7f90.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>林靖</span>
  </div>
  <div class="_2_QraFYR_0">我知道了 应该是我iproute2有问题</div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-03-30 18:06:57</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/12/ce/b4/bb5d7f90.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>林靖</span>
  </div>
  <div class="_2_QraFYR_0">倪都你好，我参考tc-bpf手册编译了一个tc程序，源码如下：<br>#include &lt;linux&#47;bpf.h&gt;<br><br>           #ifndef __section<br>           # define __section(x)  __attribute__((section(x), used))<br>           #endif<br><br>           __section(&quot;classifier&quot;) int cls_main(struct __sk_buff *skb)<br>           {<br>                   return -1;<br>           }<br><br>           char __license[] __section(&quot;license&quot;) = &quot;GPL&quot;;<br>编译完用tc加载的时候报如下错误：<br>tc filter add dev eth0 parent 1: bpf obj bcc.o verbose flowid 1:1 skip_sw<br>libbpf: loading bcc.o<br>libbpf: elf: section(3) classifier, size 24, link 0, flags 6, type=1<br>libbpf: elf: section(4) license, size 4, link 0, flags 3, type=1<br>libbpf: license of bcc.o is GPL<br>libbpf: elf: section(5) .eh_frame, size 48, link 0, flags 2, type=1<br>libbpf: elf: skipping unrecognized data section(5) .eh_frame<br>libbpf: elf: section(6) .rel.eh_frame, size 16, link 7, flags 0, type=9<br>libbpf: elf: skipping relo section(6) .rel.eh_frame for section(5) .eh_frame<br>libbpf: elf: section(7) .symtab, size 96, link 1, flags 0, type=2<br>libbpf: looking for externs among 4 symbols...<br>libbpf: collected 0 externs total<br>object file doesn&#39;t contain sec classifier<br>Unable to load program<br><br>这个问题是怎么回事，找了一下午都没有发现问题出在哪<br></div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-03-30 17:47:53</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/11/e8/f8/2c1958b6.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>yuan</span>
  </div>
  <div class="_2_QraFYR_0">老师好，我用如下命令编译静态链接的可执行文件时报错了，代码用的是github中的代码，请问有啥思路吗？<br>命令：clang -Wall -O2 -g xdp-proxy-v2.o -static -lbpf -lelf -lz -o xdp-proxy-v2<br>结果：&#47;usr&#47;bin&#47;ld: cannot find -lbpf<br>&#47;usr&#47;bin&#47;ld: cannot find -lelf<br>&#47;usr&#47;bin&#47;ld: cannot find -lz<br>&#47;usr&#47;bin&#47;ld: cannot find -lc<br>clang-13: error: linker command failed with exit code 1 (use -v to see invocation)<br><br>另外用ip link挂载也不行<br>命令：sudo ip link set dev eth0 xdpgeneric object xdp-proxy-v2.bpf.o sec xdp<br>结果：<br>BTF debug data section &#39;.BTF&#39; rejected: Invalid argument (22)!<br> - Length:       1817<br>Verifier analysis:<br>... ...<br>Prog section &#39;xdp&#39; rejected: Permission denied (13)!<br> - Type:         6<br> - Instructions: 151 (0 over limit)<br> - License:      GPL<br><br>Verifier analysis:<br>... ...<br>processed 25 insns (limit 1000000) max_states_per_insn 0 total_states 1 peak_states 1 mark_read 1<br><br>Error fetching program&#47;map!<br><br>内核版本信息如下：<br>5.14.10-300.fc35.x86_64<br>Fedora release 35 (Thirty Five)</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 缺少依赖库，参考根目录的Readme把依赖库安装一下</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-03-01 17:31:56</div>
  </div>
</div>
</div>
</li>
</ul>