<audio title="21 _ 为什么用了负载均衡更加不均衡？" src="https://static001.geekbang.org/resource/audio/42/83/42e69f12c83a80172f01132bb8f75583.mp3" controls="controls"></audio> 
<p>你好，我是胜辉。</p><p>咱们课程的第二个实战模块“应用层真实案例揭秘篇”已经进行到后半程了。前半程的四讲（15到18）都是围绕应用层特别是HTTP的相关问题展开排查。而在刚过去的两讲（19和20）里，我们又把TLS的知识和排查技巧学习了一遍。</p><p>基本上，无论是网络还是应用引发的问题，也无论是不加密的HTTP还是加密的HTTPS，你应该都已经掌握了一定的方法论和工具集，可以搞定不少问题了。</p><p>但是我们也要看到，现实世界里也有不少问题是混合型的，未必一定是跟网络有关。比如，你有没有遇到过类似下面这种问题：</p><ul>
<li>ping正常，抓包看也没有丢包或者乱序现象，但是应用就是缓慢；</li>
<li>Telnet端口能通，但应用层还是报错。</li>
</ul><p>其实这也说明了，掌握网络排查技能固然重要，但完全脱离操作系统和架构体系方面的知识，仅根据网络知识去做排查，也有可能会面临知识不够用的窘境。所以，作为一个技术人，我们<strong>任何时候都不要限制自己的学习和成长的可能</strong>，掌握得越多，相当于手里的牌越多，我们就越可能搞定别人搞不定的问题。</p><p>所以接下来的两节课，我会集中<strong>围绕系统</strong>方面的案例展开分析，希望可以帮助你构造这方面的能力。等以后你遇到网络和系统扯不清的问题时，也不会发怵，而是可以准确定位，高效推进了。</p><!-- [[[read_end]]] --><h2>案例1：高负载和不均衡</h2><p>这也是我在公有云工作的时候处理的真实案例。当时一个客户是做垂直电商的，他们的体量还不大，所以并没有自建机房，而是放到公有云上运行。他们的架构大致是这样的：</p><p><img src="https://static001.geekbang.org/resource/image/b9/01/b9956d177cdd2034011bfb13f0d97e01.jpg?wh=2000x1125" alt=""></p><p>LB运行在第四层，设置的负载均衡策略是round robin（轮询），也就是新到达LB的请求，<strong>依次派发</strong>给后端3个Nginx服务器，使得每台机器获得的请求数相同。Nginx运行在第七层，它作为Web服务器接收HTTP请求，然后通过<a href="https://en.wikipedia.org/wiki/FastCGI">FastCGI</a>接口传递给本机的<a href="https://en.wikipedia.org/wiki/PHP#PHPFPM">php-fpm</a>进程，进行应用层面的处理。</p><p>应该说，这就是一个非常典型的负载均衡架构，平时运行也正常。不过有一天，客户忽然报告系统出问题了，网站访问越来越慢，甚至经常会抛出HTTP 504错误。如下图所示：</p><p><img src="https://static001.geekbang.org/resource/image/92/d9/92114f6352e90fd60f49a4c5f647f1d9.jpg?wh=1341x435" alt="图片"></p><p>我们借助浏览器开发者工具可以看到，这个响应耗时长达60秒，然后抛出了504错误。事实上，一般人浏览一个网站的时候，等个十来秒肯定就没耐心了，所以这次的问题确实很严重。</p><h3>初步检查</h3><p>这三台Nginx服务器的配置都是8核CPU。从服务端的监控来看，它们的系统负载，也就是CPU load出现了严重的不均衡。其中一台的负载值在7左右，一台在20左右，最后一台居然高达40左右，远远超过它们的CPU核的数量。</p><p>我们知道，<strong>uptime或者top命令输出里的CPU load值，表示的是待运行的任务队列的长度</strong>。比如8核的机器，load到8，那就是8CPU核处理8个任务，全部用满了。不过，能到用满的程度，实际已经对应用的性能产生明显的影响了，所以一般建议 <strong>load值不超过核数*0.7</strong>。也就是8核的机器，建议在load达到5.6的时候，就需要重点关注了。</p><p>回到这三台Nginx机器，我们看看具体的load。</p><p>Nginx 1的load大致在10到20之间：</p><p><img src="https://static001.geekbang.org/resource/image/87/24/87f416165a50b2bcdc208e79167c2f24.png?wh=429x94" alt="图片"></p><p>Nginx 2的load在7左右：</p><p><img src="https://static001.geekbang.org/resource/image/fc/74/fcc6ab2ac1a532e9bb471f9c65f3c374.png?wh=399x95" alt="图片"></p><p>Nginx 3就很高了，在40以上：</p><p><img src="https://static001.geekbang.org/resource/image/3b/dc/3ba65448537696061a24a0d908ec3ddc.png?wh=431x65" alt="图片"></p><p>联系前面我们提到的负载均衡，你大概也明白为什么客户找过来了：因为3台机器的负载不均衡啊，难道不是你们的LB工作不正常导致的吗？</p><p>这确实是一个形式上讲得通的逻辑，我们开工吧。</p><h3>排查LB</h3><p>首先需要检查网络状况。我们做了抓包分析，发现传输本身一切正常，没有丢包也没有特殊的延迟。</p><p>然后，我们需要确认LB工作是否正常。从直观上看，处于LB后端的3台机器的load不均，似乎也意味着LB分发请求时做得不均衡，要不然为啥后端这些机器的负载各不相同呢？</p><p><img src="https://static001.geekbang.org/resource/image/66/e3/669c1f73717b9d027e4f1c048c479ae3.jpg?wh=1595x909" alt=""></p><p>不过，这个load不均的问题，也可能只是表象，因为会影响到load的因素是比较多的。我们回到最初，既然要证明LB工作是否正常，最直接的方式，还是<strong>查证LB是否做到了round robin，也就是分发的请求数量是否均等</strong>。于是我们去查LB上的日志，看看这三台后端机器分得的请求数量。</p><p><img src="https://static001.geekbang.org/resource/image/71/c3/717817cfcbb61fb9f78552ec84cf37c3.jpg?wh=300x62" alt=""></p><p>以上就是LB上的统计，红框圈出来的那一列是请求数，3台Nginx对应这3行，可见请求数都是1364，这就说明LB的round robin机制运行正常。</p><p>接着，跟之前课程里的很多案例类似，我们做了一个简单的排除性测试：我们绕过LB，直接访问Nginx机器。结果发现：</p><ul>
<li>load为7的那台机器，还勉强可以在10秒左右回复响应；</li>
<li>另外两台机器（也就是load为20和40的机器）的响应要慢很多，超过了60秒。</li>
</ul><blockquote>
<p>补充：如果只使用load为7的那台机器，而禁用另外两台，行不行呢？当然是不行的，一台肯定撑不住这个流量，必须要三台一起服务。所以还是要把根因给找到并解决。</p>
</blockquote><p>我们再来对比一下“通过LB”和“绕过LB”这两种场景下的问题现象：</p><ul>
<li>通过LB，耗时60秒的时候收到HTTP 504。</li>
<li>绕过LB，虽然收到了HTTP 200，但是耗时长达70多秒。</li>
</ul><p><img src="https://static001.geekbang.org/resource/image/38/d5/38d479ec737cccd491fe62582383a7d5.jpg?wh=2000x902" alt=""></p><p>其实这两个场景的根本问题是一致的，都是后端服务器特别慢。这就再一次排除了LB本身的嫌疑。于是，问题就变成了“<strong>为什么机器的负载变高了？</strong>”</p><h3>排查主机</h3><p>那么，到了系统排查这一步，我们就没办法再用tcpdump结合Wireshark去排查了，因为问题跟网络报文没有关系。具体点说，我们要做下面这些事情：</p><ul>
<li>分析所有进程，找到具体是哪个进程引起了load升高。</li>
<li>分析进程细节，找到是什么Bug导致该进程变成了问题进程。</li>
</ul><p>针对第一个问题，最常用的工具就是 <strong>top命令</strong>。通过top，我们很快就找到了消耗CPU资源最多的进程，发现是php-fpm进程。客户的电商程序框架本身是基于PHP开发的，所以需要PHP解释器来运行程序。又因为Nginx本身不能处理PHP，所以需要结合php-fpm才能正常工作。也就是下图这样：</p><p><img src="https://static001.geekbang.org/resource/image/cb/a3/cbb81908b9a50b951a22f1f6c7dc03a3.jpg?wh=1358x623" alt=""></p><p>既然确定了问题进程，接下来就是要排查进程导致load高的原因。排查这个问题，大致也有两个思路。</p><ul>
<li><strong>白盒检查</strong>：检查代码本身，找到根因。</li>
<li><strong>黑盒检查</strong>：不管代码怎么回事，我们从程序的外部表现来分析，寻求根因。</li>
</ul><p>因为核心代码是客户的国外合作方维护的，客户自己并不十分清楚这里面所有的逻辑，所以白盒这条路，有点超出了他们的能力。那么自然的，我们要走第二条路。</p><h3>排查操作系统</h3><p>跟网络排查中有tcpdump这样强大的工具类似，进程的排查也有相关的强大工具，比如 <strong>strace</strong>。通过strace，我们可以把排查工作从进程级别，继续追查到更细的syscall（系统调用）级别。无论是系统调用读写文件时的问题，还是系统调用本身的问题，都可以在strace的帮助下现出原形。</p><p>不过，我这里需要先介绍一些系统调用相关的知识，方便你更好地理解strace。</p><h3>什么是系统调用？</h3><p>系统调用，英文叫system call，缩写是syscall。如果我们把操作系统视作一个巨大的应用程序，那么系统调用相当于什么呢？其实，就相当于 <strong>API</strong>。利用各种系统调用，应用程序就可以做到各种跟操作系统有关的任务了。</p><p>如果没有系统调用，让应用程序直接去操作文件系统、内存等资源会怎么样呢？那一定是一场灾难。我们可以想象一个场景：程序A往地址为100~300的内存段里写入数据，然后程序B往地址为200~400的内存段里写入数据，因为200~300这段内存被A和B所共用，就很容易产生错乱，这两个程序都可能因此而出错乃至崩溃。</p><p>所以，我们必须有一个“管理机构”来统筹安排，让所有的应用程序都向这个统一机构申请资源和办理业务。<strong>这个“管理机构”就是操作系统内核，而系统调用就是这个机构提供的“服务窗口”。</strong></p><h4>内核态和用户态</h4><p>实际上，从更底层的视角来说，操作系统必须要提供系统调用的另一个原因，是计算机体系结构本身的设计。为了避免前面例子里那种不合理的操作，以及让不同安全等级的程序使用不同权限的指令集，大部分现代CPU架构都做了保护环（Protection Ring）的设计。</p><p>比如，x86 CPU实现了4个级别的保护环，也就是ring 0到ring 3，其中ring 0权限最大，ring 3最小。就拿Linux来说，它的内核态就运行在ring 0，只有内核态可以操作CPU的所有指令。Linux的用户态运行在ring 3，它就没办法操作很多核心指令。</p><p><img src="https://static001.geekbang.org/resource/image/49/b1/4987376b66aa6ee0cf56a911952073b1.jpg?wh=1455x757" alt=""></p><p>那么处于ring 3的应用程序也想要运行ring 0的指令的话，该怎么办到呢？就是让内核去间接地帮忙。而这个“忙”，其实就是通过<strong>系统调用</strong>来“帮”的。</p><p>这样的话，用户空间程序就可以借系统调用之手，完成它原本没有权限完成的指令（进入内核态）。下面这张示意图就展示了用户空间、系统调用、内核空间这三者之间的关系：</p><p><img src="https://static001.geekbang.org/resource/image/fe/e4/fe8161ea9f07ec7c8b8ba87d412b47e4.jpg?wh=2000x760" alt=""></p><p>既然，系统调用要给用户空间的应用程序提供丰富而全面的接口，无论是文件和网络等IO操作，还是像申请内存等更加核心的操作，都需要通过系统调用来完成，那么系统调用的数量肯定是不少的。比如Linux的系统调用数量大约在300多个，有的操作系统则会达到500个以上。</p><h4>strace</h4><p>了解了系统调用，我们再来认识下strace。strace这个工具的s，指的就是sycall，所以strace就是对 <strong>s</strong>yscall的trace。通过这个命令，我们可以观测到一个进程访问的所有系统调用、给这些系统调用传入的参数，以及系统调用的输出。可想而知，这样充足的信息就给系统排查工作提供了极大的帮助。</p><p>你可以想象一下，没有strace的时候，你只是看到了程序的表象，也就是程序想让你看到的，你才能看到（比如通过标准输出或者日志文件）。而有了strace，程序的一举一动就全在你的视野里了，你就像有了火眼金睛，程序在明里暗里干的所有事情，都会被你知道。</p><p>夸奖了一番strace，我们来了解一下它的具体用法。strace的用法一般有两种。</p><p><strong>直接在命令之前加上strace</strong>。比如我们想知道curl www.baidu.com这个命令，在系统调用层面具体发生了什么，就可以执行strace curl www.baidu.com，然后就能看到前后的几十个系统调用，包括打开文件的openat()、关闭文件描述符的close()、建立TCP连接的connect()等等。</p><p><strong>执行strace -p PID</strong>。这样的话，你需要先找到进程的PID，然后执行这条指令来完成追踪。这比较适合对持续运行的服务（Daemon）进行追踪。比如，你可以先找到某个进程的进程号，然后执行strace -p PID，找到这个进程在系统调用方面的细节。当然，你还可以加上各种其他参数，来达到不同的追踪效果。</p><h3>使用strace</h3><p>好了，聊完了strace的强大功能，接下来看它的表现。我们用strace命令，对php-fpm的进程号进行追踪，也就是执行这个命令：</p><pre><code class="language-bash">strace -p 进程号
</code></pre><p>果然发现一个非常奇怪的现象：整屏刷的都是gettimeofday()这个系统调用：</p><p><img src="https://static001.geekbang.org/resource/image/c9/9b/c934deec032f2442114da949d05eb89b.png?wh=338x439" alt="图片"></p><p>看起来好像只有这个调用了，那有没有别的调用呢？</p><p>我们可以对strace命令加上 <strong>-c参数</strong>，这样可以统计每个系统调用消耗的时间和次数，看看这个奇怪的gettimeofday()系统调用，占用了多少比例。我们在strace前面加上timeout 5，就可以收集5秒钟的数据了，命令如下：</p><pre><code class="language-bash">timeout 5 strace -cp 进程号
</code></pre><p>命令输出如下：</p><p><img src="https://static001.geekbang.org/resource/image/60/be/604c6160855df236940b31b6c8dff7be.png?wh=470x498" alt="图片"></p><p>可见，gettimeofday()占用了这个进程高达97.91%的运行时间。在短短的5秒之内，gettimeofday()调用次数达到了2万多次，而其他正常的系统调用，比如poll()、read()等，只有几十上百次。也就是说，<strong>非业务操作的耗时是业务操作耗时的50倍</strong>（<code>98%:2%</code>）。难怪进程这么卡，原来全都花在执行getimeofday()，也就是收集系统时间数据上了。</p><p>那么，为什么会有这么多gettimeofday()的调用呢？这里的php-fpm有什么特殊性吗？我们按这个方向去搜索，发现有不少人也遇到了同样的问题，比如Stack Overflow上这个人的<a href="https://stackoverflow.com/questions/23454686/high-cpu-usage-in-php-fpm">“遭遇”</a>。简而言之，原因多半是启用了某些性能监控软件。</p><p>我们把这个情况告诉了客户，对方忽然想起来，最近他们的国外团队确实有做一些性能监控的事情，用的软件好像叫New Relic。果然，我们在客户服务器上找到了这个New Relic。看下图：</p><p><img src="https://static001.geekbang.org/resource/image/17/09/17d0791794e05ae3a7bd242797a5dc09.png?wh=1038x92" alt="图片"></p><p>原来，问题的根因就是他们的国外团队部署了New Relic，而这个软件发起了极为频繁的gettimeofday()系统调用。这些调用抢走了大部分的CPU时间，这就导致业务代码基本没有机会被执行。因为LB有个60秒超时的机制，所以它眼看着收不到后端Nginx服务器的返回，就不得不返回HTTP 504了。也就是下面这样：</p><p><img src="https://static001.geekbang.org/resource/image/19/70/19b070a7e8070a5a2ff14246ec37fc70.jpg?wh=2000x609" alt=""></p><p>这个根因有点令人哭笑不得，不过对见怪不怪的技术支持团队来说，心里早已云淡风轻了。接下来就是客户卸载New Relic，应用立刻恢复正常。用strace再次检查，显示系统调用也恢复正常：</p><p><img src="https://static001.geekbang.org/resource/image/05/0e/05496c48587de3f23eb542d5a3c0630e.png?wh=479x496" alt="图片"></p><p>可以看到，read()和poll()系统调用，上升到进程CPU时间的60%以上，而gettimeofday()降低到不足1%，所以已经彻底解决问题了。</p><h2>案例2：LB特性和不均衡</h2><p>我再给你讲一个案例。还有一个电商客户，也遇到了一次负载不均衡的问题。这是在双十一期间，客户发起了促销活动，随之而来的访问压力的上升也十分明显，而他们的后端服务器也出现了负载不均的现象，有时候访问十分卡顿，于是我们再次介入排查。</p><p>这次的架构是一台LB后面接了两台服务器，其中一台的CPU load高达50以上，也是远远超过了8个的CPU核数。另外一台情况要好一些，load在10左右，当请求被分配到这台服务器上的时候，还算勉强可以访问。</p><p><img src="https://static001.geekbang.org/resource/image/07/c7/07602837e2c1a6c6a203762b5b37d0c7.jpg?wh=1672x513" alt=""></p><p>因为CPU load在两台机器上有比较大的差异，从访问速度来说，大约是一半请求会非常慢，一半请求略快一些，所以客户就怀疑：LB的请求分配做得不均衡，导致其中一台处理不过来。</p><p>真的是这样吗？我们检查了LB的访问日志，发现一个有意思的现象：</p><ul>
<li>大部分的访问请求是到了/api.php这个URL上。</li>
<li>按HTTP请求的源IP来分开统计，某些IP的访问量远远大于其他IP。</li>
</ul><p>我们来看一下根据源IP分开统计的访问次数：</p><p><img src="https://static001.geekbang.org/resource/image/de/aa/de51fda7a8yye48963e4bc200290ceaa.jpg?wh=500x250" alt=""></p><p>很明显，图中第一行的源IP，对/api.php这个URL有3470次访问，而其他源IP的访问量比它低了一个数量级，只有小几百。那么，这跟问题有什么关系呢？</p><p>我们通过对LB日志做进一步的分析后发现，同样是源IP的请求，要么去了Nginx 1，要么去了Nginx 2。再次检查了这台LB的配置后，我们发现客户在LB上开启了“<strong>会话保持</strong>”（Session Persistence）。这就造成了下面这种分布不均的现象：</p><p><img src="https://static001.geekbang.org/resource/image/09/b2/09c280d1644b4b9b87571c953ba767b2.jpg?wh=1625x507" alt=""></p><p>你可能也知道，会话保持是LB的常见功能，它主要是起到了维持会话状态的作用。在开启了会话保持功能后，LB会维护一张映射表，供每次分发请求之前做匹配。如果LB查到某个请求属于之前的某个会话，就会把这个请求转发给上一次会话所选择的后端服务器；否则就按默认的负载均衡算法，来选择一个后端服务器。</p><p>那么，LB怎么确定一个请求属于之前的会话呢？这就要说到会话保持的类型了。我们用的LB是HAProxy，它的会话保持有两种。</p><ul>
<li><strong>源IP</strong>：凡是源IP相同的请求，都去同一个后端服务器。</li>
<li><strong>Cookie</strong>：凡是HTTP Cookie值相同的请求，都去同一个后端服务器。</li>
</ul><p>而客户启用的是第一种：基于源IP的会话保持。不巧的是，这次访问量的源IP本身的分布就很不均匀。就像前面提到的，某个IP发起了10倍于其他IP的请求量，那么这个源IP所分配到的后端服务器，就不得不服务着10倍的请求了，然后就导致了负载不均的问题出现。</p><p>所以，我们让客户关闭了会话保持，两台后端服务器的负载很快就恢复平衡了。我们也给出了进一步的建议：最好把/api.php这个服务跟其他的服务隔离开，比如使用另外的域名，做另一套负载均衡。这是因为，/api.php的访问量和作用，跟其他接口的区别很大，通过对它做独立的负载均衡，既可以隔离互相之间的干扰，也有利于提供更加稳定的访问质量。</p><p>而且，这次的问题也是无法用tcpdump来排查的，它需要我们对LB的特点很熟悉，以及对LB和应用结合的场景，都有一个全面的认识。</p><h2>实验</h2><p>我们已经学习了strace，现在做一个简单的小实验，来巩固一下学到的知识吧。我们可以这样做：</p><ol>
<li>到<a href="https://hub.docker.com/r/moonlightysh/v_nginx">这里</a>下载并执行一个Nginx服务的Docker镜像，也就是执行：</li>
</ol><pre><code class="language-bash">docker run --cap-add=SYS_PTRACE -p 80:80 -it docker.io/moonlightysh/v_nginx bash
</code></pre><blockquote>
<p>注意，这里必须加上 <code>--cap-add=SYS_PTRACE</code>，否则容器内的strace将不能正确运行。</p>
</blockquote><ol start="2">
<li>进入容器后，启动里面的Nginx服务，执行：</li>
</ol><pre><code class="language-bash">systemctl start nginx
</code></pre><ol start="3">
<li>还是在容器内，启动strace：</li>
</ol><pre><code class="language-bash">strace -p $(pidof nginx | awk '{print $1}')
</code></pre><p>此时，strace就开始监听Nginx worker进程了。</p><blockquote>
<p>补充：这里用 <code>awk '{print $1}'</code> 是为了追踪Nginx worker进程，它出现在pidof命令输出的左边，而最右边是Nginx master进程。跟踪Nginx master进程是看不到HTTP处理过程的。</p>
</blockquote><ol start="4">
<li>从你的本机发起HTTP请求，也就是执行 <code>curl localhost:80</code>，此时在容器内Nginx在处理这次请求时，就会发起很多系统调用，而strace就全面展示了这些调用涉及的文件和参数细节。比如下面这样的strace输出：</li>
</ol><pre><code class="language-bash">root@48fc1221a03a:/# strace -p $(pidof nginx | awk '{print $1}')
strace: Process 14 attached
epoll_wait(9, [{EPOLLIN, {u32=4072472592, u64=140681431285776}}], 512, -1) = 1
accept4(6, {sa_family=AF_INET, sin_port=htons(60070), sin_addr=inet_addr("172.17.0.1")}, [112-&gt;16], SOCK_NONBLOCK) = 3
epoll_ctl(9, EPOLL_CTL_ADD, 3, {EPOLLIN|EPOLLRDHUP|EPOLLET, {u32=4072473289, u64=140681431286473}}) = 0
epoll_wait(9, [{EPOLLIN, {u32=4072473289, u64=140681431286473}}], 512, 60000) = 1
recvfrom(3, "GET / HTTP/1.1\r\nHost: localhost\r"..., 1024, 0, NULL, NULL) = 73
stat("/var/www/html/", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0
stat("/var/www/html/", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0
stat("/var/www/html/index.html", 0x7ffcb86eac80) = -1 ENOENT (No such file or directory)
stat("/var/www/html", {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0
stat("/var/www/html/index.htm", 0x7ffcb86eac80) = -1 ENOENT (No such file or directory)
stat("/var/www/html/index.nginx-debian.html", {st_mode=S_IFREG|0644, st_size=612, ...}) = 0
stat("/var/www/html/index.nginx-debian.html", {st_mode=S_IFREG|0644, st_size=612, ...}) = 0
openat(AT_FDCWD, "/var/www/html/index.nginx-debian.html", O_RDONLY|O_NONBLOCK) = 11
fstat(11, {st_mode=S_IFREG|0644, st_size=612, ...}) = 0
setsockopt(3, SOL_TCP, TCP_CORK, [1], 4) = 0
writev(3, [{iov_base="HTTP/1.1 200 OK\r\nServer: nginx/1"..., iov_len=247}], 1) = 247
sendfile(3, 11, [0] =&gt; [612], 612)&nbsp; &nbsp; &nbsp; = 612
write(4, "172.17.0.1 - - [07/Mar/2022:14:2"..., 87) = 87
close(11)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= 0
setsockopt(3, SOL_TCP, TCP_CORK, [0], 4) = 0
epoll_wait(9, [{EPOLLIN|EPOLLRDHUP, {u32=4072473289, u64=140681431286473}}], 512, 65000) = 1
recvfrom(3, "", 1024, 0, NULL, NULL)&nbsp; &nbsp; = 0
close(3)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = 0
epoll_wait(9,
</code></pre><h2>小结</h2><p>我们的课程主旨是网络排查，但因为网络跟系统又是密不可分的关系，所以在掌握Wireshark等技能以外，我们最好要熟悉操作系统排查的常规步骤。</p><p>在这节课里，我们通过一个典型的系统问题导致服务慢的案例，学习了内核空间（内核态）、用户空间（用户态）、系统调用这些操作系统的概念。另外我们还重点学习了strace这个工具，知道它有两种使用方式：<strong>直接在命令之前加上strace；执行strace -p PID。</strong></p><p>那么在案例中，我们也用 <strong>strace-cp PID中的-c参数</strong>，对进程发起的各种系统调用进行了执行次数和执行时间的统计，这对于分析进程耗时的去向会很有帮助。</p><p>在这里，我们也再一次温习下HTTP 504的概念。在后端服务不能在LB的时限内回复HTTP响应的时候，LB就会用HTTP 504来回复给客户端，告诉它是后端服务超时（潜台词：我LB可没问题）。</p><p>而在第二个案例里，我们了解了<strong>会话保持在某些情况下会“放大”负载不均衡</strong>的问题。所以你要知道，在启用会话保持功能时，你需要根据实际情况，做通盘的考虑。</p><h2>思考题</h2><p>给你留两道思考题：</p><ul>
<li>在案例1里面，LB回复了HTTP 504。那在什么情况下，这个LB会回复HTTP 503呢？</li>
<li>除了strace，你还知道哪些trace类的工具可以帮助排查呢？</li>
</ul><p>欢迎你在留言区分享自己的经验，我们一同成长。</p>
<style>
    ul {
      list-style: none;
      display: block;
      list-style-type: disc;
      margin-block-start: 1em;
      margin-block-end: 1em;
      margin-inline-start: 0px;
      margin-inline-end: 0px;
      padding-inline-start: 40px;
    }
    li {
      display: list-item;
      text-align: -webkit-match-parent;
    }
    ._2sjJGcOH_0 {
      list-style-position: inside;
      width: 100%;
      display: -webkit-box;
      display: -ms-flexbox;
      display: flex;
      -webkit-box-orient: horizontal;
      -webkit-box-direction: normal;
      -ms-flex-direction: row;
      flex-direction: row;
      margin-top: 26px;
      border-bottom: 1px solid rgba(233,233,233,0.6);
    }
    ._2sjJGcOH_0 ._3FLYR4bF_0 {
      width: 34px;
      height: 34px;
      -ms-flex-negative: 0;
      flex-shrink: 0;
      border-radius: 50%;
    }
    ._2sjJGcOH_0 ._36ChpWj4_0 {
      margin-left: 0.5rem;
      -webkit-box-flex: 1;
      -ms-flex-positive: 1;
      flex-grow: 1;
      padding-bottom: 20px;
    }
    ._2sjJGcOH_0 ._36ChpWj4_0 ._2zFoi7sd_0 {
      font-size: 16px;
      color: #3d464d;
      font-weight: 500;
      -webkit-font-smoothing: antialiased;
      line-height: 34px;
    }
    ._2sjJGcOH_0 ._36ChpWj4_0 ._2_QraFYR_0 {
      margin-top: 12px;
      color: #505050;
      -webkit-font-smoothing: antialiased;
      font-size: 14px;
      font-weight: 400;
      white-space: normal;
      word-break: break-all;
      line-height: 24px;
    }
    ._2sjJGcOH_0 ._10o3OAxT_0 {
      margin-top: 18px;
      border-radius: 4px;
      background-color: #f6f7fb;
    }
    ._2sjJGcOH_0 ._3klNVc4Z_0 {
      display: -webkit-box;
      display: -ms-flexbox;
      display: flex;
      -webkit-box-orient: horizontal;
      -webkit-box-direction: normal;
      -ms-flex-direction: row;
      flex-direction: row;
      -webkit-box-pack: justify;
      -ms-flex-pack: justify;
      justify-content: space-between;
      -webkit-box-align: center;
      -ms-flex-align: center;
      align-items: center;
      margin-top: 15px;
    }
    ._2sjJGcOH_0 ._10o3OAxT_0 ._3KxQPN3V_0 {
      color: #505050;
      -webkit-font-smoothing: antialiased;
      font-size: 14px;
      font-weight: 400;
      white-space: normal;
      word-break: break-word;
      padding: 20px 20px 20px 24px;
    }
    ._2sjJGcOH_0 ._3klNVc4Z_0 {
      display: -webkit-box;
      display: -ms-flexbox;
      display: flex;
      -webkit-box-orient: horizontal;
      -webkit-box-direction: normal;
      -ms-flex-direction: row;
      flex-direction: row;
      -webkit-box-pack: justify;
      -ms-flex-pack: justify;
      justify-content: space-between;
      -webkit-box-align: center;
      -ms-flex-align: center;
      align-items: center;
      margin-top: 15px;
    }
    ._2sjJGcOH_0 ._3Hkula0k_0 {
      color: #b2b2b2;
      font-size: 14px;
    }
</style><ul><li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/10/eb/09/ba5f0135.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>Chao</span>
  </div>
  <div class="_2_QraFYR_0">503 服务不可用。 反代无法到达业务服务商。</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 是的，如果反向代理或者LB找不到可用的后端服务（比如向后端的健康检查都是失败的），就向前端请求回复503~<br>这里也说一下502,503,504的区别：<br>502：LB收到了后端的无效回复，可以参考前面的第17讲的案例<br>503：LB明确的知道服务不可用，LB不会转发请求给后端，而是直接向前端回复503<br>504：LB转发了请求给后端，但后端没有在时限内返回，到了时间点LB就向前端回复504</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-03-09 10:17:35</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="http://thirdwx.qlogo.cn/mmopen/vi_32/Q0j4TwGTfTKmXoYDRv98a9GEoJccTovtWH928eN9N6ZN0fibiamEVbpGwpAzuRmhEbI2sUZa6Hur7utDYMXYrIHw/132"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>小白</span>
  </div>
  <div class="_2_QraFYR_0">第一个案例按照道理top -c 本身就能定位到问题所在。</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 你好，这个问题在于，当时我们并不了解php-fpm，更不知道newrelic意味着什么，所以是先找问题进程（Php-fpm），然后找它具体在做什么（定位到98%的时间在做gettimeofday)，从而推理出这个进程在收集性能数据，最后才想到是newrelic。这是一个推理的过程（前提就是不知道newrelic是什么）。那么推理出来的结论（newrelic引起性能问题），也可以成为经验，下次看到newrelic的存在，可以先排查是否是它引起的问题~</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-03-10 14:45:38</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/11/8f/cf/890f82d6.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>那时刻</span>
  </div>
  <div class="_2_QraFYR_0">老师提到两个案例，都曾遇到过。不过当时我的服务被new relic搞得cpu飙升，老师提供的strace是个很好的工具。 </div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 嗯，我们做这个案例前，也不知道newrelic是什么。strace因为是大部分linux机器默认自带的，所以直接用就好了。如果觉得系统调用的信息还不够，还想查到内核内部的函数调用链，那就要用到ftrace了</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-03-09 10:05:46</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/1b/e4/79/0f0114ba.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>taochao_zs</span>
  </div>
  <div class="_2_QraFYR_0">1 503是表示后端服务不可用，后端respone返回404或500，LB这层应该就会解析成503；<br>2 perf工具排查业务调用函数cpu分布，动态追踪systemtab；</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 其实503的语义来说，还是根据后端服务可用性来判断的，如果LB认为后端已经不可用，就会回复503。如果后端回复404，500，LB会原样转发的：）<br>嗯perf和systemtap的补充也很好~</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-03-09 09:49:26</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/10/7f/d3/b5896293.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>Realm</span>
  </div>
  <div class="_2_QraFYR_0">问题一：<br>503错误，估计是后端服务已经不可用了，不是慢的问题了，比如程序挂了、被ddos攻击、高负载被熔断了；<br><br>问题二：<br>strace -p真是香，可以追踪系统调用，查看调用栈，可以定位很多问题。<br>类似的这种动态追踪的技术，还有perf，eBPF<br><br>谢谢老师的分享！<br></div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 嗯，关于502，503，504的区别，我这里也补充一下：<br>502：LB收到了后端的无效回复，可以参考前面的第17讲的案例<br>503：LB明确的知道服务不可用，LB不会转发请求给后端，而是直接向前端回复503<br>504：LB转发了请求给后端，但后端没有在时限内返回，到了时间点LB就向前端回复504<br><br>strace一般情况下够用了，如果要更详细到内核本身，除了你说的perf, eBPF，还可以用ftrace<br><br>也感谢你的支持：）</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-03-09 07:59:39</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/2d/4f/5d/f0f3d02f.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>兔极生威</span>
  </div>
  <div class="_2_QraFYR_0">作者能说说lb的直接访问和绕开的部署方式吗？还有为什么strace -p phppid你能看到其他进程的系统调用呢</div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2023-02-16 14:06:32</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/16/cd/db/7467ad23.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>Bachue Zhou</span>
  </div>
  <div class="_2_QraFYR_0">New Relic 也是很有名的软件了，怎么会有这种低级问题。。</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 回头看这个问题并不复杂，所以重在整理分析思路。也许这次是newrelic引起问题，下次是别的什么引起问题了：）</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-11-03 09:06:11</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/10/47/00/3202bdf0.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>piboye</span>
  </div>
  <div class="_2_QraFYR_0">timeout 竟然又学到个bash 命令， 佩服老师</div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-08-06 21:09:06</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/18/4b/d7/f46c6dfd.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>William Ning</span>
  </div>
  <div class="_2_QraFYR_0">Cool, keep learning~~</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 加油！</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-05-11 17:06:19</div>
  </div>
</div>
</div>
</li>
<li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/16/b4/94/2796de72.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>追风筝的人</span>
  </div>
  <div class="_2_QraFYR_0">1.   503是服务端故障  服务不可用<br>2. strace, dig, queryperf , traceroute</div>
  <div class="_10o3OAxT_0">
    
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2022-03-18 10:20:16</div>
  </div>
</div>
</div>
</li>
</ul>