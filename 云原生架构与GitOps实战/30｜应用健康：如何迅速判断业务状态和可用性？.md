<audio title="30｜应用健康：如何迅速判断业务状态和可用性？" src="https://static001.geekbang.org/resource/audio/d6/48/d682f32036a7c744d0fab8c6e803b348.mp3" controls="controls"></audio> 
<p>你好，我是王炜。</p><p>从这节课开始，我们正式进入云原生架构的全新领域：应用可观测性。</p><p>在生产环境下，我们经常会被问到这些问题：业务现在健康吗？线上部署的是什么版本，是在什么时候部署的？它的整体可用性怎么样？是否有报错信息等等。在单体应用架构下，这些问题可能非常容易回答，但是在分布式的微服务架构下，要迅速回答这些问题并不简单。</p><p>从概念的定义上来说，可观测性包括三个方面，也就是我们常说的指标、日志和链路。指标主要用于衡量程序性能，通过指标的度量值我们可以判断系统的表现情况。最常见的指标有 CPU、内存、磁盘和网络等。日志主要用来统一收集业务输出的日志，包括各种级别的提示警告和错误日志信息等，结合查询系统我们便能够快速定位错误。链路一般指的是分布式追踪，它可以评估一个完整的请求链上微服务的性能情况。</p><p>不过，可观测性更多的是深入到微服务内部监控性能和指标，而在实际的业务场景中，当业务出现异常时，我们一般会从外到内排查问题。也就是先检查应用整体的状态和可用性，再借助可观测性工具对应用内每一个微服务进行深入排查。</p><p>所以，在正式进入到可观测性的学习之前，这节课，我会先从排查问题的第一原则出发，也就是从最外层的业务应用出发，介绍在 GitOps 场景下判断业务状态、查找故障的几种方法。当你在实践中遇到生产故障时，完全可以把这节课的内容作为故障排查手册来使用。</p><!-- [[[read_end]]] --><h2>应用健康状态</h2><p>对于 Kubernetes 应用而言，它往往会包含不同的 Kubernetes 对象和工作负载，要判断应用是否处于健康状态，等同于判断应用所有的工作负载是否都处于健康状态，这很繁琐并且也不直观。</p><p>我之前之所以推荐使用 ArgoCD 来部署应用，其中一个很重要的原因就是它内置了应用级的<strong>“健康状态”</strong>，如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/7c/b7/7c39db2067c0b887746bf233ef4be0b7.png?wh=1920x204" alt="图片"></p><p>ArgoCD 为几种标准的 Kubernetes 对象提供了健康状态的算法，当应用内所有资源都处于健康状态时，就认为应用也处于健康状态，这非常符合我们判断应用状态的标准。</p><p>对于 Kubernetes 的工作负载，例如 Deployment、StatefulSet 和 DaemonSet 等，有下面三个判断健康状态的条件。</p><ol>
<li>工作负载处于运行状态。</li>
<li>部署的版本符合期望版本。</li>
<li>实际的副本数符合期望的副本数。</li>
</ol><p>对于 Service 来说，ArgoCD 会检查类型是否为 LoadBalancer ，并判断 status.loadBalancer.ingress 字段是否为空，从而判断是否成功创建了负载均衡器。而对于 Ingress ，则判断 status.loadBalancer.ingress 字段是否为空。</p><p>最后，对于持久卷（PVC），ArgoCD 会判断 status.phase 字段是否为 Bound 状态。</p><p>所以，要检查业务应用的健康状态，你应该首先查看 ArgoCD 的应用健康状态。如果状态是 Heathy，说明应用运行正常，如果状态是 Degraded，说明应用处于异常状态。</p><p>此外，ArgoCD 的 “CURRENT SYNC STATUS” 还能够帮助我们进一步判断集群内所有资源是否已经和仓库进行了同步。Synced 状态表示已经同步完成，Out-Of-Sync 代表集群资源和 Git 仓库有差异，这时候就需要留意产生差异的原因，例如是否手动修改了集群内的资源或者直接在集群内创建了新的资源等。</p><h2>Pod 健康状态</h2><p>如果 ArgoCD 的应用状态为 Degraded，代表应用状态异常，这时候就需要进一步排查，并从 Kubernetes 对象开始着手了。</p><p>在之前的课程中，我们提到 Pod 是 Kubernetes 调度的最小单位。所以，当工作负载出现故障时，我们首先应该查看 Pod 的状态。</p><p>Pod 的状态涉及到启动状态和运行状态，排查问题也相对复杂。接下来，我们通过一个例子来说明一下排查方法。</p><p>首先，我们创建用来实验的 Pod，将下面的内容保存为 pod-status.yaml 文件。</p><pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: running
  labels:
    app: nginx
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          protocol: TCP

---
apiVersion: v1
kind: Pod
metadata:
  name: backoff
spec:
  containers:
    - name: web
      image: nginx:not-exist

---
apiVersion: v1
kind: Pod
metadata:
  name: error
spec:
  containers:
    - name: web
      image: nginx
      command: ["sleep", "a"]
</code></pre><p>然后，使用 kubectl apply 命令将它应用到集群内。</p><pre><code class="language-yaml">$ kubectl apply -f pod-status.yaml
pod/running created
pod/backoff created
pod/error created
</code></pre><p>接下来，使用 kubectl get pods 查看刚才创建的 3 个 Pod。</p><pre><code class="language-yaml">NAME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;READY&nbsp; &nbsp;STATUS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RESTARTS&nbsp; &nbsp; &nbsp; &nbsp; AGE
backoff&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0/1&nbsp; &nbsp; &nbsp;ImagePullBackOff&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;50s
error&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0/1&nbsp; &nbsp; &nbsp;CrashLoopBackOff&nbsp;  1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4s
running&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;50s
</code></pre><p>在这个例子中，我们一共创建了 3 个 Pod，它们的状态包含 ImagePullBackOff、CrashLoopBackOff 和 Running。</p><p>Running 状态表示运行中，除此之外，剩下两种异常状态分别代表<strong>启动和运行阶段的错误</strong>，它们也是生产环境出现频率最高的错误，接下来我会对它们继续深入分析。</p><h3>ImagePullBackOff</h3><p>ImagePullBackOff 是一个典型的<strong>容器启动错误</strong>，除它之外，你可能还会看到下面这几个容器启动阶段的错误：</p><ul>
<li>ErrImagePull</li>
<li>ImageInspectError</li>
<li>ErrImageNeverPull</li>
<li>RegistryUnavailable</li>
<li>InvalidImageName</li>
</ul><p>这几种错误出现的概率比较低，并且单纯从字面上也就能大概理解它的含义所以这里我主要介绍 ImagePullBackOff 错误。它可能是下面这两个原因导致的。</p><ol>
<li>镜像名称或者版本错误，在我们刚才创建的 backoff Pod 中，我们指定的镜像为 nginx:not-exist，但是实际上镜像版本 not-exist 并不存在，自然也就会抛出错误。</li>
<li>指定了私有镜像，但又没有提供拉取凭据。</li>
</ol><p>有的时候，你可能看到了错误，但很难立刻找到具体的原因，这里我为你总结一个方法。对于启动阶段的错误，我们可以使用 kubectl describe 命令来查看错误的详情。</p><pre><code class="language-yaml">$ kubectl describe pod backoff
Events:
&nbsp; Type&nbsp; &nbsp; &nbsp;Reason&nbsp; &nbsp; &nbsp;Age&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; From&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Message
&nbsp; ----&nbsp; &nbsp; &nbsp;------&nbsp; &nbsp; &nbsp;----&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;----&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-------
&nbsp; Normal&nbsp; &nbsp;Scheduled&nbsp; 10m&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; default-scheduler&nbsp; Successfully assigned default/backoff to kind-control-plane
&nbsp; Normal&nbsp; &nbsp;Pulling&nbsp; &nbsp; 8m43s (x4 over 10m)&nbsp; kubelet&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Pulling image "nginx:not-exist"
&nbsp; Warning&nbsp; Failed&nbsp; &nbsp; &nbsp;8m40s (x4 over 10m)&nbsp; kubelet&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Failed to pull image "nginx:not-exist": rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/nginx:not-exist": failed to resolve reference "docker.io/library/nginx:not-exist": docker.io/library/nginx:not-exist: not found
&nbsp; Warning&nbsp; Failed&nbsp; &nbsp; &nbsp;8m40s (x4 over 10m)&nbsp; kubelet&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Error: ErrImagePull
&nbsp; Warning&nbsp; Failed&nbsp; &nbsp; &nbsp;8m11s (x6 over 10m)&nbsp; kubelet&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Error: ImagePullBackOff
&nbsp; Normal&nbsp; &nbsp;BackOff&nbsp; &nbsp; 4s (x42 over 10m)&nbsp; &nbsp; kubelet&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Back-off pulling image "nginx:not-exist"
</code></pre><p>从返回结果里 Event 事件中的第三行我们可以发现，集群抛出了 nginx:not-exist: not found 的异常，这样我们也就定位到了具体的错误。</p><h3>CrashLoopBackOff</h3><p>CrashLoopBackOff 是一种典型的容器<strong>运行阶段的错误</strong>。除此之外，你可能还会看到类似的 RunContainerError 错误。</p><p>出现这个错误的原因主要有下面两个。</p><ol>
<li>容器内的应用程序在启动时出现了错误，例如配置读取失败导致无法启动。</li>
<li>配置出错，例如配置了错误的容器启动命令。</li>
</ol><p>在上面创建的 error Pod 的例子中，我故意错误地配置了容器的启动命令，这样我们也就看到了 CrashLoopBackOff 异常。</p><p>对于运行阶段的错误，大部分错误都来源于业务本身的启动阶段，所以，我们只需要查看 Pod 的日志一般就能够找到问题所在。比如，我们尝试来查看 error Pod 的日志。</p><pre><code class="language-yaml">$ kubectl logs error
sleep: invalid time interval 'a'
Try 'sleep --help' for more information.
</code></pre><p>从返回的日志来看，sleep 命令抛出了一个异常，也就是参数错误。在生产环境下，我们一般会用 Deoloyment 工作负载来管理 Pod，在 Pod 出现运行阶段异常的情况下，Pod 名称会随着重新启动而出现变化，这时候你可以在查看日志时增加 --previous 参数，以此查看之前的 Pod 日志。</p><pre><code class="language-yaml">$ kubectl logs pod-name --previous
</code></pre><h3>Pending</h3><p>有时候，你可能不会看到启动和运行的错误状态，但查看状态时，会看到 Pod 处于 Pending 状态。你可以尝试将下面的内容保存为 pending-pod.yaml 文件，并通过 kubectl apppy -f 将这个例子部署到集群内。</p><pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pending
spec:
  containers:
    - name: web
      image: nginx
      resources:
        requests:
          cpu: 32
          memory: 64Gi
</code></pre><p>接下来，尝试查看 Pod 的状态。</p><pre><code class="language-yaml">$ kubectl get pods
NAME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;READY&nbsp; &nbsp;STATUS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RESTARTS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;AGE
pending&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0/1&nbsp; &nbsp; &nbsp;Pending&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 15s
</code></pre><p>从返回结果我们会发现，Pod 没有抛出任何异常，但它的状态处于 Pending，同时 READY 0/1 表示 Pod 没有准备好接收外部流量。</p><p>出现 Pending 状态主要的原因可能有下面三种。</p><ol>
<li>集群资源不足以调度 Pod。</li>
<li>Pod 正在等待 PVC 持久化存储卷。</li>
<li>Pod 资源用量超过了命名空间的资源配额。</li>
</ol><p>在上面的例子中，我们为 Pod 配置了 32 核 64G 的资源请求配额，这显然超出了集群资源。此时 Pod 会处于 Pending 状态，并且 Kubernetes 会一直尝试调度，一旦加入了新的节点并满足资源要求，Pod 就会被重新启动。</p><p>Pending 状态其实也算是容器启动异常的一种情况，但它并不能算是错误，只是暂时无法调度。要查明 Pending 状态的具体原因，你可以参考寻找容器启动错误的方法，通过 kubectl describe 命令来查看。</p><pre><code class="language-yaml">$ kubectl describe pod pending
Events:
&nbsp; Type&nbsp; &nbsp; &nbsp;Reason&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Age&nbsp; &nbsp; From&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Message
&nbsp; ----&nbsp; &nbsp; &nbsp;------&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ----&nbsp; &nbsp;----&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-------
&nbsp; Warning&nbsp; FailedScheduling&nbsp; 11m&nbsp; &nbsp; default-scheduler&nbsp; 0/1 nodes are available: 1 Insufficient cpu, 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.
&nbsp; Warning&nbsp; FailedScheduling&nbsp; 6m45s&nbsp; default-scheduler&nbsp; 0/1 nodes are available: 1 Insufficient cpu, 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.
</code></pre><p>从返回结果的 Event 事件中我们可以得出结论，Pending 出现的原因是没有符合 CPU 资源条件的 Node 节点，Kubernetes 尝试调度了两次，异常情况相同。</p><h2>Service 连接状态</h2><p>有时候，即便是 Pod 处于运行且处于就绪状态，我们也无法从外部请求到业务服务。这时候就要关注 Service 的连接状态了。</p><p>Service 是 Kubernetes 的核心组件，正常情况下它都是可用的。在生产环境下，流量的流向一般是从 Ingress 到 Service 再到 Pod。所以，当无法在外部访问到 Pod 的业务服务时，我们可以先从最内层也就是 Pod 开始检查，最简单的方式就是直连 Pod 并发起请求，查看 Pod 是否能够正常工作。</p><p>要在本地访问 Pod，我们可以使用 kubectl port-forward 进行端口转发，以我们刚才创建的 Nginx Pod 为例。</p><pre><code class="language-yaml">$ kubectl port-forward pod/running 8081:80
</code></pre><p>如果在本地访问 8081 端口请求能够成功，则代表 Pod 和业务层面是正常的。</p><p>接下来，我们进一步检查 Service 的连接状态。同样地，最简单的方式也是通过端口转发直连 Service 发起请求。</p><pre><code class="language-yaml">$ kubectl port-forward service/&lt;service-name&gt; local_port:service_pod
</code></pre><p>如果请求 Service 能够正确返回内容，说明 Service 这一层也是正常的。如果无法返回内容，这时候通常可能有两个原因。</p><ol>
<li>Service Selector 选择器没有正确匹配到 Pod。</li>
<li>Service 的 Port 和 TargetPort 配置错误。</li>
</ol><p>通过修复这两项配置，你应该就能修复 Service 到 Pod 的连接问题了。</p><h2>Ingress 连接状态</h2><p>到这里，如果仍然无法从 Ingress 访问业务服务，那么就需要继续排查 Ingress 了。</p><p>首先，确认 Ingress 控制器的 Pod 是否处于运行状态。</p><pre><code class="language-yaml">$ kubectl get pods -n ingress-nginx
NAME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; READY&nbsp; &nbsp;STATUS&nbsp; &nbsp; &nbsp; RESTARTS&nbsp; &nbsp; &nbsp; &nbsp; AGE
ingress-nginx-controller-8544b85788-c9m2g&nbsp; &nbsp;1/1&nbsp; &nbsp; &nbsp;Running&nbsp; &nbsp; &nbsp;6 (4h35m ago)&nbsp; &nbsp;1d
</code></pre><p>在确认 Ingress 控制器并无异常之后，基本上可以确认是 Ingress 策略配置错误导致的故障了。</p><p>你可以通过 kubectl describe ingress 命令来查看 Ingress 策略。</p><pre><code class="language-yaml">$ kubectl describe ingress ingress_name
Name:             ingress_name
Namespace:        default
Rules:
  Host        Path  Backends
  ----        ----  --------
              /     running-service:80 (&lt;error: endpoints "running-service" not found&gt;)
</code></pre><p>如果结果中出现 not found，那么修正配置即可修复 Ingress 访问的问题了。</p><h2>总结</h2><p>这节课，我向你简单介绍了可观测性，它们包括指标、日志和链路。不过，在带你实践可观测性之前，我们需要先了解应用状态的判定标准以及排除常见故障的方法。</p><p>在判断应用健康状态方面，我们可以借助 ArgoCD 控制台的<strong>“应用健康”</strong>来快速了解业务的可用性。业务应用往往是由很多不同的 Kubernetes 对象组成的，ArgoCD 会对它们进行综合判断得到应用健康状态，这使我们不再需要手动查看每一个资源的状态。</p><p>当应用处于不健康的状态时，我们就需要进一步深入到 Kubernetes 对象中查找具体的原因了。</p><p>在这么多的工作负载类型中，Pod 的健康状态是我们首先要查看的。它可能在<strong>启动和运行阶段</strong>产生故障，这两个故障类型的典型的代表是 ImagePullBackOff 和 CrashLoopBackOff。根据我的生产经验，这两种故障大部分情况下是镜像名和版本错误，或者业务进程自身启动异常导致的。要查找具体的原因，你只需要记住两条命令（kubectl logs 和 kubectl describe pod ）就可以满足大部分的场景了。</p><p>Pod 的 Pending 状态则相对特殊，它并不是错误，而是调度层面的限制导致的。例如资源配额不足或者等待 PVC 绑定等。</p><p>最后，如果 Pod 处于运行和就绪状态，但是仍然无法从集群外部访问业务系统，我们就需要进一步查看 Service 和 Ingress 了，这一般是由<strong>配置错误</strong>导致的。通常，你可以先将 Pod 进行端口转发，并尝试在本地直接访问 Pod，如果访问正常，那么就可以进一步排查 Service 和 Ingress 配置，例如查看 Service 选择器和端口配置，以及 Ingress 指向后端 Service 的配置。</p><p>在下一节课，我将会介绍可观测性的日志方向，并带你从零搭建一个实时的日志系统。</p><h2>思考题</h2><p>最后，给你留一道思考题吧。</p><p>如何在不借助 Ingress 外网 IP 的情况下，调试完整的请求链路（从域名-&gt;Ingress-&gt;Service-&gt;Pod 完整的链路）？</p><p>欢迎你给我留言交流讨论，你也可以把这节课分享给更多的朋友一起阅读。我们下节课见。</p>
<style>
    ul {
      list-style: none;
      display: block;
      list-style-type: disc;
      margin-block-start: 1em;
      margin-block-end: 1em;
      margin-inline-start: 0px;
      margin-inline-end: 0px;
      padding-inline-start: 40px;
    }
    li {
      display: list-item;
      text-align: -webkit-match-parent;
    }
    ._2sjJGcOH_0 {
      list-style-position: inside;
      width: 100%;
      display: -webkit-box;
      display: -ms-flexbox;
      display: flex;
      -webkit-box-orient: horizontal;
      -webkit-box-direction: normal;
      -ms-flex-direction: row;
      flex-direction: row;
      margin-top: 26px;
      border-bottom: 1px solid rgba(233,233,233,0.6);
    }
    ._2sjJGcOH_0 ._3FLYR4bF_0 {
      width: 34px;
      height: 34px;
      -ms-flex-negative: 0;
      flex-shrink: 0;
      border-radius: 50%;
    }
    ._2sjJGcOH_0 ._36ChpWj4_0 {
      margin-left: 0.5rem;
      -webkit-box-flex: 1;
      -ms-flex-positive: 1;
      flex-grow: 1;
      padding-bottom: 20px;
    }
    ._2sjJGcOH_0 ._36ChpWj4_0 ._2zFoi7sd_0 {
      font-size: 16px;
      color: #3d464d;
      font-weight: 500;
      -webkit-font-smoothing: antialiased;
      line-height: 34px;
    }
    ._2sjJGcOH_0 ._36ChpWj4_0 ._2_QraFYR_0 {
      margin-top: 12px;
      color: #505050;
      -webkit-font-smoothing: antialiased;
      font-size: 14px;
      font-weight: 400;
      white-space: normal;
      word-break: break-all;
      line-height: 24px;
    }
    ._2sjJGcOH_0 ._10o3OAxT_0 {
      margin-top: 18px;
      border-radius: 4px;
      background-color: #f6f7fb;
    }
    ._2sjJGcOH_0 ._3klNVc4Z_0 {
      display: -webkit-box;
      display: -ms-flexbox;
      display: flex;
      -webkit-box-orient: horizontal;
      -webkit-box-direction: normal;
      -ms-flex-direction: row;
      flex-direction: row;
      -webkit-box-pack: justify;
      -ms-flex-pack: justify;
      justify-content: space-between;
      -webkit-box-align: center;
      -ms-flex-align: center;
      align-items: center;
      margin-top: 15px;
    }
    ._2sjJGcOH_0 ._10o3OAxT_0 ._3KxQPN3V_0 {
      color: #505050;
      -webkit-font-smoothing: antialiased;
      font-size: 14px;
      font-weight: 400;
      white-space: normal;
      word-break: break-word;
      padding: 20px 20px 20px 24px;
    }
    ._2sjJGcOH_0 ._3klNVc4Z_0 {
      display: -webkit-box;
      display: -ms-flexbox;
      display: flex;
      -webkit-box-orient: horizontal;
      -webkit-box-direction: normal;
      -ms-flex-direction: row;
      flex-direction: row;
      -webkit-box-pack: justify;
      -ms-flex-pack: justify;
      justify-content: space-between;
      -webkit-box-align: center;
      -ms-flex-align: center;
      align-items: center;
      margin-top: 15px;
    }
    ._2sjJGcOH_0 ._3Hkula0k_0 {
      color: #b2b2b2;
      font-size: 14px;
    }
</style><ul><li>
<div class="_2sjJGcOH_0"><img src="https://static001.geekbang.org/account/avatar/00/12/32/a8/d5bf5445.jpg"
  class="_3FLYR4bF_0">
<div class="_36ChpWj4_0">
  <div class="_2zFoi7sd_0"><span>郑海成</span>
  </div>
  <div class="_2_QraFYR_0">我理解问题可以认为和下面的说法一致。如何在本地机房通过域名暴露服务?<br>1.service和pod配置方法和有外网IP是一样的，就不重复说了<br>2.Ingress在云厂商环境下会提供外网的LB-VIP，在本地机房环境则没有，需要解决一下。方法想到两个：方法一，Ingress-controller的service由loadbalancer改为nodeport，这样就可以使用集群node的IP替代VIP，缺点是nodeport是一个大端口还可能会变；方法二，使用metallb等其他lb方案为Ingress-controller service提供VIP这样在大二层网络通的情况下就可以访问了<br>3.域名解析在云厂商环境是通过其提供的DNS解析到LB-VIP实现，本地机房可以使用基础网络里的自建DNS实现或者host文件实现</div>
  <div class="_10o3OAxT_0">
    <p class="_3KxQPN3V_0">作者回复: 👍🏻两种方法都可以实现暴露对外暴露，metallb 的方案可能会更好一些。</p>
  </div>
  <div class="_3klNVc4Z_0">
    <div class="_3Hkula0k_0">2023-02-20 09:10:41</div>
  </div>
</div>
</div>
</li>
</ul>